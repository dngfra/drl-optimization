# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SjH7SLZuhPFiD00JbG2pf4iOT_EFBiPP
"""

import keras
import numpy as np
import gym 
import pandas as pd
from keras.layers import Dense 
from keras.models import Sequential 


def crossover_function(agents, rewards):
  '''
  models: a list of keras neural networks (parents agents)
  rewars: list (or array) of rewards associated to the performance of the 
          corresponding model
  return: the child weights computed by the weighted average
          of the parents w.r.t. the reward
  '''
  rewards = np.array(rewards)
  num_layers = len(agents[0].model.get_weights())
  normalized_rewards = rewards/np.sum(rewards)
  child_model = []
  for i in range(num_layers):
    new_layer = np.zeros_like(agents[0].model.get_weights()[i])
    for j,parent_agent in enumerate(agents):
      layer = parent_agent.model.get_weights()[i] * normalized_rewards[j]
      new_layer = new_layer + layer
    child_model.append(new_layer)
  return child_model
  
  
def generate_population(child_model, num_children, agents, scale_noise=0.1):
  '''
  child_model: model from which building the new population
  num_children: number of children to generate
  scale_noise: variance of the gaussian noise to apply
  agents: list of agents 
  '''
  new_children = []
  for child in range(num_children):
    new_child = []
    for layer in child_model:
      new_layer = np.random.normal(layer, scale_noise)
      new_child.append(new_layer)
  #Ho fatto questa piccola modifica per avere direttamente una lista di agenti che è quello che poi ci servirebbe piuttosto che una lista di modelli ma non sono sicuro che funzioni
    agents[child].model.set_weights(new_child)
    #new_children.append(Agent(state_size, action_size,new_child))
  return agents


def crossover_function_2(agents, rewards):
  '''
  models: a list of keras neural networks (parents agents)
  rewars: list (or array) of rewards associated to the performance of the
          corresponding model
  return: the child weights computed by the weighted average
          of the parents w.r.t. the reward
  '''
  sorted_indeces = np.argsort(-rewards)
  best_agent = agents[sorted_indeces[0]]

  second_best = agents[sorted_indeces[1]]
  return best_agent, second_best


def generate_population_2(child_model1, child_model2, num_children, scale_noise=0.01):
  '''
  child_model: model from which building the new population
  num_children: number of children to generate
  scale_noise: variance of the gaussian noise to apply
  '''

  new_children = []
  for child in range(num_children/2-1):
    new_child = []
    for layer in child_model1.model.get_weights():
      new_layer = np.random.normal(layer, scale_noise)
      new_child.append(new_layer)
      new_children.append(Agent(state_size, action_size, new_child))

  for child in range(num_children/2-1):
    new_child = []
    for layer in child_model2.model.get_weights():
      new_layer = np.random.normal(layer, scale_noise)
      new_child.append(new_layer)
      new_children.append(Agent(state_size, action_size, new_child))
  new_children.append(child_model1)
  new_children.append(child_model2)
  #Ho fatto questa piccola modifica per avere direttamente una lista di agenti che è quello che poi ci servirebbe piuttosto che una lista di modelli ma non sono sicuro che funzioni
  return new_children

class Agent:
  
    def __init__(self, state_size, action_size, weights=None):
        # if you want to see Cartpole learning, then change to True
        self.render = False
        self.load_model = False
        # get size of state and action
        self.state_size = state_size
        self.action_size = action_size
        self.value_size = 1
        # create model for policy network
        self.model = self.build_model()
        if weights is not None: 
          self.model.set_weights(weights)
          
    def build_model(self):
      model = Sequential()
      model.add(Dense(24, input_dim=self.state_size, activation='relu',
                      kernel_initializer='he_uniform'))
      model.add(Dense(self.action_size, activation='softmax',
                      kernel_initializer='he_uniform'))
      #model.summary()
      return model

    def get_action(self, state):
        policy = self.model.predict(state, batch_size=1).flatten()
        #secondo me non ha molto senso fare una multinomial perchè ora non stiamo trainando niente quindi è solo una cosa deterministica che sceglie l'azione in base alla policy, dall'esploration non trarrebbe effettivamente nessun vantaggio se non eventualmente un punteggio più alto dovuto alla casualità del sampling e che quindi non rispecchia proprimanete la policy e lo stesso per un punteggio più basso. 
        #per questo prenderei soltanto l'azione con la probabilità maggiore
        #return np.random.choice(self.action_size, 1, p=policy)[0]
        return np.argmax(policy)

N = 100
max_time = 1000
n_generations = 100
# In case of CartPole-v1, maximum length of episode is 500
env = gym.make('CartPole-v1')
env._max_episode_steps = 1000
#score_logger = ScoreLogger('CartPole-v1')
# get size of state and action from environment
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

agents = []
#scores è quello che nella funzione crossover abbiamo chiamo rewards forse bisogna rinominarli
scores = []
for i in range(N):
  agent = Agent(state_size, action_size)
  agents.append(agent)

#ho rimosso la tupla perchè probabilmente ci basta solo una lista di agent e poi resettiamo sempre lo stesso ambiente
mean_score_gen = []
variance_score_gen = []
max_score_gen = []

for i in range(n_generations):
  print("generation: ", str(i))
  scores = []
  for agent in agents:
    #agents_list.append(agent)
    done = False
    score = 0
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    while (not done) and (score < max_time):
        if agent.render:
          env.render()

        action = agent.get_action(state)
        next_state, reward, done, info = env.step(action)
        next_state = np.reshape(next_state, [1, state_size])
        score += reward
        state = next_state

        if done:
          #print(score)
          scores.append(score)
          state = env.reset()
          #sys.exit()
  print(np.mean(scores))
  print(np.max(scores))
  mean_score_gen.append(np.mean(scores))
  variance_score_gen.append(np.var(scores))
  max_score_gen.append(np.max(scores))
  #Ore creiamo il genitore della prossima generazione a partire dai risultati di quella precedente e sostituiamo la lista agents 
  parent = crossover_function(agents,scores)
  agents = generate_population(parent,N, agents,0.1)
data = pd.DataFrame({'mean': mean_score_gen,'variance': variance_score_gen,'max': max_score_gen})
data.to_csv('data')
